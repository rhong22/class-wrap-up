{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e43de",
   "metadata": {},
   "source": [
    "# Natural language processing: The bag-of-words algorithm \n",
    "\n",
    "files needed = ('reviews.csv', 'newsgroups.zip')\n",
    "\n",
    "In this lecture we're going to shift gears from dealing with numerical data to text data. \n",
    "\n",
    "Working with text as data is known as Natural Language Processing (NLP). A common use of NLP is categorizing a set of text. Perhaps the most ubiquitous example is a spam filter. It reads the text of a message and determines if it is \"spam\" or \"ham.\" Another common use case is to deduce the sentiment of a block of text. Is the earnings call positive or negative? Is the Fed statement hawkish or dovish? Is the review positive or negative? NLP models form the basis for automatic translation software and voice recognition. Lots of cool stuff.  \n",
    "\n",
    "We'll employ one simple NLP algorithm known as the *bag-of-words* algorithm to deduce the sentiment of a dataset of movie reviews. There are more sophisticated methods but this will give us the big idea. \"More sophisticated\" typically amounts to tweaks on how we process the data or more complex classifier (discrete) models.\n",
    "\n",
    "Our work today falls under the category of *supervised* machine learning, in that we are training the algorithm/estimating the model on *labeled* data where the true classification is known. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2b79b",
   "metadata": {},
   "source": [
    "## NLP vs. classifiers\n",
    "\n",
    "Natural language is harder for algorithms to interpret and analyze than numeric data since:\n",
    "\n",
    "1. Sentences are not of fixed lengths, but most algorithms require a standard input vector size.\n",
    "\n",
    "2. Most algorithms cannot understand words as input, so each word needs to be represented by some numeric value.\n",
    "\n",
    "So our method is:\n",
    "\n",
    "1. Pre-processing: Clean up the text. \n",
    "2. Estimate a model on the training data: Let $\\text{word}_{ji}$ be the number of times the word $j$ occurs in review $i$\n",
    "$$\\text{Pr}(\\text{review}_{i}=1|\\text{words}) = \\text{logit}(\\beta_{0} + \\beta_{1}\\text{word}_{1i}+ \\beta_{2}\\text{word}_{2i}+\\cdots)$$ \n",
    "3. Use the estimated model to classify new reviews.\n",
    "\n",
    "*I'm using the machine-learning term \"classifier\" in this notebook, but everywhere you see the word \"classifier,\" you can replace it with \"Logit\" and nothing changes. Similarly, you can replace the word \"features\" with \"exogenous variables.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347b4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9aaa7",
   "metadata": {},
   "source": [
    "## A simple example with SPAM\n",
    "Spam emails/messages belong to the broad category of unsolicited messages received by a user. Spam occupies unwanted space and bandwidth, amplifies the threat of viruses, and in general exploits a user's connection to social networks. Plus, they're annoying.\n",
    "\n",
    "Three messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8e7067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Number</th>\n",
       "      <th>Text of Documents</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text 1</td>\n",
       "      <td>You have won a prize. Call today to claim.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text 2</td>\n",
       "      <td>It is your mother. Call me.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text 3</td>\n",
       "      <td>Are you around today? I need a favor from you.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document Number                               Text of Documents  Label\n",
       "0          Text 1      You have won a prize. Call today to claim.      1\n",
       "1          Text 2                     It is your mother. Call me.      0\n",
       "2          Text 3  Are you around today? I need a favor from you.      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus is a fancy word for a collection (or a body) of text.\n",
    "# Label marks a message as spam (1) or not spam (0).\n",
    "\n",
    "corpus = [('Text 1', 'You have won a prize. Call today to claim.', 1),\n",
    "          ('Text 2', 'It is your mother. Call me.', 0),\n",
    "          ('Text 3', 'Are you around today? I need a favor from you.', 1)]\n",
    "data = pd.DataFrame(corpus, columns=['Document Number','Text of Documents', 'Label'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acb956",
   "metadata": {},
   "source": [
    "Even though python is good with text, we will still need to convert our text into  numeric data to get a classifier model to analyze it. Let's create a matrix with the word counts. Each row of the matrix is an observation (a message) and each column is a word. The cells in the matrix are the number of times that word is found in the message.\n",
    "\n",
    "The scikit package gives us the `CountVectorizer` to do this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3058726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
      " [0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1]\n",
      " [1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['Text of Documents'])\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350b4ca",
   "metadata": {},
   "source": [
    "Note that `X` is an array-like object. We are in the realm of scikit, which doesn't use DataFrames. Let's turn this back into a DataFrame, though, so we can see things clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71caef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>around</th>\n",
       "      <th>call</th>\n",
       "      <th>claim</th>\n",
       "      <th>favor</th>\n",
       "      <th>from</th>\n",
       "      <th>have</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>me</th>\n",
       "      <th>mother</th>\n",
       "      <th>need</th>\n",
       "      <th>prize</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>won</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Document Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Text 1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text 3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 are  around  call  claim  favor  from  have  is  it  me  \\\n",
       "Document Number                                                            \n",
       "Text 1             0       0     1      1      0     0     1   0   0   0   \n",
       "Text 2             0       0     1      0      0     0     0   1   1   1   \n",
       "Text 3             1       1     0      0      1     1     0   0   0   0   \n",
       "\n",
       "                 mother  need  prize  to  today  won  you  your  \n",
       "Document Number                                                  \n",
       "Text 1                0     0      1   1      1    1    1     0  \n",
       "Text 2                1     0      0   0      0    0    0     1  \n",
       "Text 3                0     1      0   0      1    0    2     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = vectorizer.get_feature_names_out()\n",
    "\n",
    "exog = pd.DataFrame(X.toarray(), columns=cols, index=data['Document Number'])\n",
    "\n",
    "exog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072080ad",
   "metadata": {},
   "source": [
    "The `exog` DataFrame contains our features and the `data['Label']` column contains our outcome variable. We now have the data ready  to estimate a classifier model (e.g., a logit regression).\n",
    "\n",
    "$$\\text{Pr(Label=1|exog)} = \\Lambda( \\beta_0 + \\beta_1\\text{are} + \\beta_1\\text{are} + \\beta_1\\text{around} + \\beta_1\\text{call} + \\cdots)$$\n",
    "\n",
    "In statsmodels, this would be:\n",
    "\n",
    "```\n",
    "sm.Logit(data['label'], exog).fit()\n",
    "```\n",
    "\n",
    "This dataset is too small to actually fit a model, so let's move on to something bigger. \n",
    "\n",
    "\n",
    "Note that this methodology of turning text into data is not limited to classification problems. For example, we could use this approach to connect stock performance with Federal Open Market Committee (FOMC) statements to predict how the Federal Reserve's statements on the economy influence the S&P500, the Dow Jones, individual stocks, and government treasury prices. NLP is a broad topic and a lot of fun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87724bf3",
   "metadata": {},
   "source": [
    "## The new package\n",
    "\n",
    "Text data have a whole set of problems to deal with: misspelling, different versions of words, capitalization.\n",
    "We will use the *natural language tool kit* (nltk) to help us process the text data. It comes with anaconda, but if you need to install it: \n",
    "\n",
    "```python\n",
    "pip install --user nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8550338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8395e8",
   "metadata": {},
   "source": [
    "## Detecting sentiment\n",
    "\n",
    "The dataset that we are using is a large movie review dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/). It contains positive and negative reviews, in English. There are two columns. The first column tells us whether the review is positive (1) or negative (0). The second column corresponds to the text of the review. \n",
    "\n",
    "The full dataset at the above source contains 50,000 reviews, half of them positive, if you'd like to play around with these methods more. The authors below also created separate train and test sets with a disjoint set of movies so that performance wouldn't be juiced upward by movie-specific terms like \"Chewbacca\" or \"Romy\". We'll skip the train/test distinction in the below, but note that model performance should be tested out-of-sample. We'll work with a 10% random sample.\n",
    "\n",
    "Source reference: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616c125b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>As much as I hate to disagree with the origina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Between sweeping, extraordinary scenes within ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the ultimate, and I mean the ULTIMATE,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Recognizing the picture of the diner on the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I have nothing at all against Paul Schrader. I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  As much as I hate to disagree with the origina...\n",
       "1      1  Between sweeping, extraordinary scenes within ...\n",
       "2      1  This is the ultimate, and I mean the ULTIMATE,...\n",
       "3      0  Recognizing the picture of the diner on the co...\n",
       "4      0  I have nothing at all against Paul Schrader. I..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('reviews.csv', usecols=['label','review'])\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f8170",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "This is the part that makes nlp different from working with numeric data. We need to clean up the text and turn it into a feature matrix. \n",
    "\n",
    "```\n",
    "'I am helping raise $100 for UW Madison'                     original\n",
    "'i am helping raise $100 for uw madison'                     homogenize capitalization\n",
    "'i am helping raise for uw madison'                          remove non-alphabetic characters\n",
    "['i', 'am', 'helping', 'raise',  'for', 'uw', 'madison']     tokenize\n",
    "['helping', 'raise', 'uw', 'madison']                        remove stop words\n",
    "['help', 'raise', 'uw', 'madison']                           stem and lem\n",
    "'help raise uw madison'                                      back to a single string\n",
    "```\n",
    "\n",
    "Then create the feature matrix\n",
    "\n",
    "| help | raise | uw | madison|\n",
    "|------|-------|----|--------|\n",
    "| 1    | 1     |  1 |     1  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b65bd6",
   "metadata": {},
   "source": [
    "### 1. Homogenize the capitalization\n",
    "\n",
    "We don't want to worry about 'Hello' not being equal to 'hello'. Let's make everything lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba3e2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>as much as i hate to disagree with the origina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>between sweeping, extraordinary scenes within ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the ultimate, and i mean the ultimate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>recognizing the picture of the diner on the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i have nothing at all against paul schrader. i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  as much as i hate to disagree with the origina...\n",
       "1      1  between sweeping, extraordinary scenes within ...\n",
       "2      1  this is the ultimate, and i mean the ultimate,...\n",
       "3      0  recognizing the picture of the diner on the co...\n",
       "4      0  i have nothing at all against paul schrader. i..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Homogenize capitalization\n",
    "dataset['review'] = dataset['review'].str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09926c80",
   "metadata": {},
   "source": [
    "### 2. Remove non-alphabetic characters\n",
    "\n",
    "Our algorithm will only use words to characterize reviews. This is not strictly necessary, but can help sometimes. \n",
    "\n",
    "We will remove them using a regular expression. \n",
    "\n",
    "As a reminder, the code to remove the non-alphabetic characters is \n",
    "```python\n",
    "dataset['review'].str.replace('[^A-Za-z]', ' ', regex=True)\n",
    "```\n",
    "\n",
    "The regex part is the `'[^A-Za-z]'`. It says: \"find everything that is not the letters A through Z or a through z.\" We replace the non-alphabetic stuff with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29eba1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>as much as i hate to disagree with the origina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>between sweeping  extraordinary scenes within ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this is the ultimate  and i mean the ultimate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>recognizing the picture of the diner on the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i have nothing at all against paul schrader  i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  as much as i hate to disagree with the origina...\n",
       "1      1  between sweeping  extraordinary scenes within ...\n",
       "2      1  this is the ultimate  and i mean the ultimate ...\n",
       "3      0  recognizing the picture of the diner on the co...\n",
       "4      0  i have nothing at all against paul schrader  i..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Remove non-alphabetic characters\n",
    "dataset['review'] = dataset['review'].str.replace('[^A-Za-z]', ' ', regex=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a73c0a",
   "metadata": {},
   "source": [
    "### 3. Tokenize the strings\n",
    "\n",
    "Break the strings up into lists of words, which are easier to process. This is very similar to using `.str.split(' ')`. Here we use the tokenizer method from nltk. It is a bit more sophisticated than a simple split.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize as wt \n",
    "```\n",
    "\n",
    "We also need to download the punctuation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b895b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashleyswanson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[as, much, as, i, hate, to, disagree, with, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[between, sweeping, extraordinary, scenes, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[this, is, the, ultimate, and, i, mean, the, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[recognizing, the, picture, of, the, diner, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[i, have, nothing, at, all, against, paul, sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  [as, much, as, i, hate, to, disagree, with, th...\n",
       "1      1  [between, sweeping, extraordinary, scenes, wit...\n",
       "2      1  [this, is, the, ultimate, and, i, mean, the, u...\n",
       "3      0  [recognizing, the, picture, of, the, diner, on...\n",
       "4      0  [i, have, nothing, at, all, against, paul, sch..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Tokenize the strings.\n",
    "\n",
    "# Get the punctuation. \n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize as wt \n",
    "dataset['review'] = dataset['review'].apply(wt)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af3319",
   "metadata": {},
   "source": [
    "### 4. Removing stop words\n",
    "\n",
    "Now we eliminate *stop words*&mdash;words in the text that add no specific meaning. They often involve prepositions, helping verbs, and articles (e.g., in, the, an, is). Since these add no value to our model, let's get rid of them.\n",
    "\n",
    "Fortunately, linguists have already identified stopwords specific to each language, so we can readily identify and exclude them.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "stop_wrds = stopwords.words('english')\n",
    "```\n",
    "\n",
    "`stop_wrds` is a list of English-language stop words. \n",
    "\n",
    "We need to loop through the lists and check for stop words. I will write a small function that does the looping and then apply it to the DataFrame's column using `.apply()`.\n",
    "\n",
    "Again, we need to download the stopwords first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc28669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashleyswanson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[much, hate, disagree, original, poster, found...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[sweeping, extraordinary, scenes, within, plet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[ultimate, mean, ultimate, adventure, classic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[recognizing, picture, diner, cover, dvd, made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[nothing, paul, schrader, fact, hardcore, one,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  [much, hate, disagree, original, poster, found...\n",
       "1      1  [sweeping, extraordinary, scenes, within, plet...\n",
       "2      1  [ultimate, mean, ultimate, adventure, classic,...\n",
       "3      0  [recognizing, picture, diner, cover, dvd, made...\n",
       "4      0  [nothing, paul, schrader, fact, hardcore, one,..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Remove stop words.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_wrds = stopwords.words('english')\n",
    "\n",
    "def remove_stops(x):\n",
    "    temp = []\n",
    "    for word in x:\n",
    "        if word not in stop_wrds:\n",
    "            temp.append(word)\n",
    "    return temp\n",
    "\n",
    "dataset['review'] = dataset['review'].apply(remove_stops)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f79c3",
   "metadata": {},
   "source": [
    "### 5. Stemming and lemmatization\n",
    "Words like act, actor, and acting are all versions of the same root word (act). **Stemming** and **lemmatization** are techniques used to truncate words in order to get the stem or the base word. The difference between these two methods is that after stemming, the stem may not be an actual word, whereas lemmatization always produces a real word, which results in better interpretation of the corpora by humans.\n",
    "\n",
    "For example, studies could be stemmed as studi (not a word), but will be lemmatized as study (an existing word). To be honest, this feels like a rabbit hole, so I'm treating this stuff as a black box and trusting that the linguists are doing a good job.\n",
    "\n",
    "Let's stem these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2958e2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[much, hate, disagre, origin, poster, found, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[sweep, extraordinari, scene, within, plethora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[ultim, mean, ultim, adventur, classic, plot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[recogn, pictur, diner, cover, dvd, made, real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[noth, paul, schrader, fact, hardcor, one, fav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  [much, hate, disagre, origin, poster, found, a...\n",
       "1      1  [sweep, extraordinari, scene, within, plethora...\n",
       "2      1  [ultim, mean, ultim, adventur, classic, plot, ...\n",
       "3      0  [recogn, pictur, diner, cover, dvd, made, real...\n",
       "4      0  [noth, paul, schrader, fact, hardcor, one, fav..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Stemming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_it(x):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in x]\n",
    "\n",
    "dataset['review'] = dataset['review'].apply(stem_it) \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581cd97",
   "metadata": {},
   "source": [
    "That seemed like a lot of work, but it always does when we are first learning something. Putting all the code together, the processing is simply: \n",
    "\n",
    "```python\n",
    "dataset['review'] = dataset['review'].str.lower()\n",
    "dataset['review'] = dataset['review'].str.replace('[^A-Za-z]', ' ', regex=True)\n",
    "dataset['review'] = dataset['review'].apply(wt)\n",
    "dataset['review'] = dataset['review'].apply(remove_stops)\n",
    "dataset['review'] = dataset['review'].apply(stem_it) \n",
    "```\n",
    "\n",
    "You could even wrap all that up in a function, too...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774d58e",
   "metadata": {},
   "source": [
    "### Create the feature matrix\n",
    "\n",
    "We are done pre-processing. \n",
    "\n",
    "1. Turn the lists of words back into strings.\n",
    "2. Create the feature matrix using `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0672a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['review'] = dataset['review'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc48da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The matrix of word counts. I am limiting the feature matrix to 250 columns.\n",
    "\n",
    "X = CountVectorizer(max_features=250).fit_transform(dataset['review'])\n",
    "exog = pd.DataFrame(X.toarray())\n",
    "\n",
    "# The outcome data.\n",
    "endog = dataset['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea54a81",
   "metadata": {},
   "source": [
    "### Estimate the logit model\n",
    "\n",
    "I'm using the scikit learn package to estimate the logit model. This is estimating the same logit model as we did with statsmodels, but scikit learn has a faster implementation. \n",
    "\n",
    "The syntax is a bit different, but it's the same idea as what we saw in class Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679d35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7b8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rev = LogisticRegression(random_state=0, max_iter=1000).fit(exog, endog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bf83b",
   "metadata": {},
   "source": [
    "scikit does not have a `.summary()` function like statsmodels.  In this kind of application, we are not concerned with the individual coefficients. We don't really care *why* a word is good at classifying a positive review, we just care that the model works.  That is, we care about prediction, not identification of an underlying relationship. \n",
    "\n",
    "You can still recover all the parameters, etc., but not in a neat table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73568d",
   "metadata": {},
   "source": [
    "How is our in-sample fit? Compare the predicted values to the actual values. The `.score()` method computes the share of the reviews properly categorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e5ac0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.4% of sample reviews were properly categorized.\n"
     ]
    }
   ],
   "source": [
    "print('{0:.1f}% of sample reviews were properly categorized.'.format(res_rev.score(exog, endog)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a048f2",
   "metadata": {},
   "source": [
    "The confusion matrix is (from the [docs](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.MultinomialResults.pred_table.html)):\n",
    "\n",
    "```\n",
    "pred_table[i,j] refers to the number of times “i” was observed and the model predicted “j”. \n",
    "Correct predictions are along the diagonal.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd01ba85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1014</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1014   215\n",
       "1   175  1096"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix(endog, res_rev.predict(exog))\n",
    "pd.DataFrame(confusion_matrix(endog, res_rev.predict(exog)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e7c34",
   "metadata": {},
   "source": [
    "We have 175 false negatives (14%), and 215 false positives (17%). Better than random, but performance may improve with a larger dataset and more features. May also be helpful to strip out proper names (of actors/directors/writers/titles) in pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd586fd",
   "metadata": {},
   "source": [
    "## Top Hat Practice Exercise: Newsgroups\n",
    "\n",
    "We're going to practice using the \"20 Newsgroup\" data set, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his \"Newsweeder: Learning to filter netnews\" paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "The goal is to categorize each post based on its content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba8256",
   "metadata": {},
   "source": [
    "1. Download, unzip, and load the file 'newsgroups.csv'. Only import the first 500 rows. Try the `nrows` option of `.read_csv()`. \n",
    "\n",
    "   `article` is the message. `category code` is the newsgroup category code. `category` is the newsgroup category name. \n",
    "   \n",
    "   **Our goal:** Create a classifier that predicts the category code of an article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d887c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category code</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  category code  \\\n",
       "0  I was wondering if anyone out there could enli...              7   \n",
       "1  A fair number of brave souls who upgraded thei...              4   \n",
       "2  well folks, my mac plus finally gave up the gh...              4   \n",
       "3  \\nDo you have Weitek's address/phone number?  ...              1   \n",
       "4  From article <C5owCB.n3p@world.std.com>, by to...             14   \n",
       "\n",
       "                category  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('newsgroups.csv', nrows=500)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42475e05",
   "metadata": {},
   "source": [
    "2. How many articles are there in each category. Looks like it's time for `.groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29a999a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "alt.atheism                 26\n",
       "comp.graphics               23\n",
       "comp.os.ms-windows.misc     36\n",
       "comp.sys.ibm.pc.hardware    18\n",
       "comp.sys.mac.hardware       34\n",
       "comp.windows.x              22\n",
       "misc.forsale                27\n",
       "rec.autos                   21\n",
       "rec.motorcycles             29\n",
       "rec.sport.baseball          28\n",
       "rec.sport.hockey            25\n",
       "sci.crypt                   20\n",
       "sci.electronics             30\n",
       "sci.med                     30\n",
       "sci.space                   27\n",
       "soc.religion.christian      32\n",
       "talk.politics.guns          19\n",
       "talk.politics.mideast       26\n",
       "talk.politics.misc          18\n",
       "talk.religion.misc           9\n",
       "Name: article, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#news['category'].value_counts()\n",
    "news.groupby('category')['article'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd4876",
   "metadata": {},
   "source": [
    "3. Process the text data. All the code to do this is gathered in the cell above the **Create feature matrix** heading above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d19635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category code</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[wonder, anyon, could, enlighten, car, saw, da...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fair, number, brave, soul, upgrad, si, clock,...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[well, folk, mac, plu, final, gave, ghost, wee...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[weitek, address, phone, number, like, get, in...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[articl, c, owcb, n, p, world, std, com, tomba...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  category code  \\\n",
       "0  [wonder, anyon, could, enlighten, car, saw, da...              7   \n",
       "1  [fair, number, brave, soul, upgrad, si, clock,...              4   \n",
       "2  [well, folk, mac, plu, final, gave, ghost, wee...              4   \n",
       "3  [weitek, address, phone, number, like, get, in...              1   \n",
       "4  [articl, c, owcb, n, p, world, std, com, tomba...             14   \n",
       "\n",
       "                category  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['article'] = news['article'].str.lower()\n",
    "news['article'] = news['article'].str.replace('[^A-Za-z]', ' ', regex=True)\n",
    "news['article'] = news['article'].apply(wt)\n",
    "news['article'] = news['article'].apply(remove_stops)\n",
    "news['article'] = news['article'].apply(stem_it) \n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750080e",
   "metadata": {},
   "source": [
    "4. Turn the lists of words in `article` into strings. \n",
    "5. Create the feature matrix. Allow it to use 100 features.\n",
    "6. Create the outcome variable. (the Series that contains the category codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58743c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['article'] = news['article'].str.join(' ')\n",
    "X = CountVectorizer(max_features=100).fit_transform(news['article'])\n",
    "exog = pd.DataFrame(X.toarray())\n",
    "\n",
    "# The outcome data.\n",
    "endog = news['category code']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f1741",
   "metadata": {},
   "source": [
    "7. Estimate the logit model.\n",
    "8. Use `.score()` to check the in-sample fit.\n",
    "9. Compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e356ebba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_news = LogisticRegression(random_state=0, max_iter=1000).fit(exog, endog)\n",
    "res_news.score(exog, endog)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422efd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
       "0   20   1   0   0   0   0   0   0   0   5   0   0   0   0   0   0   0   0   \n",
       "1    2  14   1   0   0   0   1   0   0   1   2   0   1   0   1   0   0   0   \n",
       "2    0   0  29   0   0   0   1   1   1   2   0   0   1   1   0   0   0   0   \n",
       "3    0   0   0  15   0   0   0   1   0   1   1   0   0   0   0   0   0   0   \n",
       "4    0   1   3   0  21   1   0   0   0   4   0   0   2   1   0   0   0   0   \n",
       "5    0   1   0   0   2  16   0   0   0   2   0   0   1   0   0   0   0   0   \n",
       "6    0   0   0   0   1   0  25   0   0   1   0   0   0   0   0   0   0   0   \n",
       "7    0   1   0   0   0   0   1  14   0   2   0   0   3   0   0   0   0   0   \n",
       "8    0   0   0   0   1   1   0   0  22   3   1   0   0   0   0   0   0   1   \n",
       "9    0   0   0   0   1   0   0   0   0  23   1   0   0   1   1   0   0   1   \n",
       "10   1   0   1   0   1   0   1   1   0   2  17   0   0   0   0   0   0   1   \n",
       "11   1   0   0   0   0   0   0   0   0   2   0  16   0   0   1   0   0   0   \n",
       "12   0   1   0   0   1   0   0   0   0   0   0   1  21   1   3   0   0   0   \n",
       "13   2   0   0   0   1   0   0   1   0   0   0   0   2  22   1   0   0   1   \n",
       "14   0   0   1   1   0   0   0   0   1   3   0   0   1   0  20   0   0   0   \n",
       "15   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0  30   0   0   \n",
       "16   0   0   0   0   0   0   0   0   1   1   0   0   0   1   0   1  15   0   \n",
       "17   2   0   0   0   1   0   0   0   0   3   0   0   0   0   0   0   0  20   \n",
       "18   0   0   0   0   0   0   1   0   1   1   0   0   0   0   0   0   0   0   \n",
       "19   1   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   \n",
       "\n",
       "    18  19  \n",
       "0    0   0  \n",
       "1    0   0  \n",
       "2    0   0  \n",
       "3    0   0  \n",
       "4    1   0  \n",
       "5    0   0  \n",
       "6    0   0  \n",
       "7    0   0  \n",
       "8    0   0  \n",
       "9    0   0  \n",
       "10   0   0  \n",
       "11   0   0  \n",
       "12   2   0  \n",
       "13   0   0  \n",
       "14   0   0  \n",
       "15   0   0  \n",
       "16   0   0  \n",
       "17   0   0  \n",
       "18  15   0  \n",
       "19   0   6  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(endog, res_news.predict(exog)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4d710",
   "metadata": {},
   "source": [
    "10. Go back to step 1. and increase the number of rows to 1000. Rerun your code. Does the accuracy improve?\n",
    "\n",
    "    Go back to step 5. and add more features to your exogenous variables. Does the accuracy improve?\n",
    "    \n",
    "    Do you see any patterns in the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35ecf944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('newsgroups.csv', nrows=1000)\n",
    "\n",
    "news['article'] = news['article'].str.lower()\n",
    "news['article'] = news['article'].str.replace('[^A-Za-z]', ' ', regex=True)\n",
    "news['article'] = news['article'].apply(wt)\n",
    "news['article'] = news['article'].apply(remove_stops)\n",
    "news['article'] = news['article'].apply(stem_it) \n",
    "\n",
    "news['article'] = news['article'].str.join(' ')\n",
    "X = CountVectorizer(max_features=100).fit_transform(news['article'])\n",
    "exog = pd.DataFrame(X.toarray())\n",
    "\n",
    "# The outcome data.\n",
    "endog = news['category code']\n",
    "res_news = LogisticRegression(random_state=0, max_iter=1000).fit(exog, endog)\n",
    "res_news.score(exog, endog)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98e1609e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('newsgroups.csv', nrows=1000)\n",
    "\n",
    "news['article'] = news['article'].str.lower()\n",
    "news['article'] = news['article'].str.replace('[^A-Za-z]', ' ', regex=True)\n",
    "news['article'] = news['article'].apply(wt)\n",
    "news['article'] = news['article'].apply(remove_stops)\n",
    "news['article'] = news['article'].apply(stem_it) \n",
    "\n",
    "news['article'] = news['article'].str.join(' ')\n",
    "X = CountVectorizer(max_features=200).fit_transform(news['article'])\n",
    "exog = pd.DataFrame(X.toarray())\n",
    "\n",
    "# The outcome data.\n",
    "endog = news['category code']\n",
    "res_news = LogisticRegression(random_state=0, max_iter=1000).fit(exog, endog)\n",
    "res_news.score(exog, endog)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "973b8d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
       "0   47   1   0   0   1   0   0   0   0   3   0   0   0   1   0   0   0   1   \n",
       "1    0  37   0   1   2   0   2   0   1   0   0   0   1   1   0   0   0   0   \n",
       "2    0   0  45   2   1   0   1   0   1   2   1   0   1   0   0   0   0   0   \n",
       "3    1   0   1  36   3   0   0   0   1   1   1   0   1   0   0   0   0   0   \n",
       "4    0   0   0   1  51   1   0   0   1   1   0   0   1   0   1   0   0   1   \n",
       "5    0   1   2   0   4  36   1   0   0   2   0   0   0   0   0   0   0   0   \n",
       "6    0   0   0   0   0   0  52   0   0   2   1   0   1   0   0   0   0   0   \n",
       "7    0   0   0   0   1   0   1  45   0   2   0   0   3   0   0   0   0   0   \n",
       "8    0   0   0   0   1   1   1   0  37   2   2   0   5   0   0   0   0   0   \n",
       "9    1   0   0   0   0   0   0   0   0  51   1   0   0   0   0   0   0   1   \n",
       "10   0   0   0   0   0   0   2   0   2   3  40   0   0   0   0   0   0   0   \n",
       "11   0   0   0   0   0   0   0   0   1   2   0  42   0   0   0   0   0   0   \n",
       "12   0   0   0   0   0   0   0   1   0   3   0   0  47   0   0   0   1   0   \n",
       "13   0   0   1   0   1   1   1   1   1   2   0   0   0  46   0   0   0   0   \n",
       "14   0   0   1   1   0   1   0   0   3   2   0   0   2   0  40   0   0   0   \n",
       "15   0   0   0   0   0   0   0   0   0   3   0   0   1   0   0  65   0   0   \n",
       "16   0   0   0   0   1   0   0   0   0   4   0   0   0   0   0   0  36   0   \n",
       "17   2   0   0   0   1   0   0   0   0   2   0   0   0   0   0   0   0  49   \n",
       "18   0   0   0   0   0   0   1   0   0   2   1   0   0   1   0   1   0   0   \n",
       "19   0   0   0   0   0   0   1   0   1   1   0   0   0   0   0   2   0   0   \n",
       "\n",
       "    18  19  \n",
       "0    0   0  \n",
       "1    0   0  \n",
       "2    1   0  \n",
       "3    0   0  \n",
       "4    0   0  \n",
       "5    1   0  \n",
       "6    0   0  \n",
       "7    1   0  \n",
       "8    0   0  \n",
       "9    0   0  \n",
       "10   0   0  \n",
       "11   1   0  \n",
       "12   1   0  \n",
       "13   0   0  \n",
       "14   0   0  \n",
       "15   0   0  \n",
       "16   0   0  \n",
       "17   0   0  \n",
       "18  34   0  \n",
       "19   0  25  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(endog, res_news.predict(exog)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91b9572c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category code</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>doug robert ken hill nl mvp let go spo</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   article  category code            category\n",
       "26  doug robert ken hill nl mvp let go spo              9  rec.sport.baseball"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news['category code']==9].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3e86e",
   "metadata": {},
   "source": [
    "## Want more?\n",
    "\n",
    "This notebook was meant to give you an idea of how to turn words into data. We used a very simple \"bag-of-words\" model where we only considered how often a word appears. There are more complex methods (term frequency-inverse document frequency, ngrams,...) and alternative modeling (machine learning methods). \n",
    "\n",
    "If you want to learn more, Dan Jurafsky and James H. Martin have a [free book online](https://web.stanford.edu/~jurafsky/slp3/) that I think is quite good. As always, you can google \"nlp tutorial\" and see what is out there. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
